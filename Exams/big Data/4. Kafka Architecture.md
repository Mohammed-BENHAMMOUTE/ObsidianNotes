# Real-Time Streaming: Kafka and Flume - Exam Study Guide

## Apache Kafka

### What is Apache Kafka?
- **Distributed commit log service** widely used for data ingestion
- Conceptually similar to **publish-subscribe messaging system**
- Offers **scalability, performance, reliability, and flexibility**

### Key Use Cases
- Log aggregation
- Messaging
- Website activity tracking
- Stream processing
- Event sourcing

### Core Terminology
- **<mark style="background: #ADCCFFA6;">Message</mark>**: Single data record passed by Kafka
- **<mark style="background: #ADCCFFA6;">Topic</mark>**: Named log or feed of messages within Kafka
- <mark style="background: #ADCCFFA6;">Producer</mark>: Program that writes messages to Kafka
- **<mark style="background: #ADCCFFA6;">Consumer</mark>**: Program that reads messages from Kafka

### Messages
- **Variable-size byte arrays** representing arbitrary user-defined content
- Common formats: free-form text, JSON, Avro
- **No explicit size limit** but optimal performance at few KB per message
- **Practical limit: 1MB per message**
- Kafka retains all messages for <mark style="background: #ABF7F7A6;">defined time period and/or total size</mark>

### Topics
- No explicit limit on number of topics
- **Kafka works better with few large topics than many small ones**
- Can be created explicitly or by publishing to topic (configurable behavior)
- **Recommendation**: Disable auto-creation to avoid accidental topic creation

### Producers
- Publish messages to Kafka topics
- Communicate with **Kafka, not consumers**
- Kafka **persists messages to disk** on receipt

### Consumers
- Read messages published to Kafka topics
- Communicate with **Kafka, not producers**
- **Consumer actions don't affect other consumers**
- Can come and go without impact on cluster or other consumers

### Topic Partitioning
- **Kafka divides each topic into partitions** for improved scalability and throughput
- **Topic partition = ordered and immutable sequence of messages**
- New messages appended as received
- Each message assigned unique sequential ID called **offset**

### Consumer Groups
- **One or more consumers** form consumer group to work together
- **Each partition consumed by only one member** of consumer group
- **Message ordering preserved per partition, NOT across topic**

### Kafka Clusters
- Consists of **one or more brokers** (servers running Kafka broker daemon)
- **Depends on Apache ZooKeeper** for coordination

### Apache ZooKeeper
- **Coordination service for distributed applications**
- Kafka uses ZooKeeper for:
  - Keeping track of brokers in cluster
  - Detecting addition/removal of consumers
- Typically runs **3 or 5 ZooKeeper instances**

### Kafka Brokers
- **Fundamental daemons** that make up Kafka cluster
- Fully stores topic partition on disk with memory caching
- Single broker can host **1000 topic partitions**
- **One broker elected as controller** (for partition assignment)
- Each daemon runs in **own JVM**
- Single machine can run multiple broker daemons

### Topic Replication
- Set **replication count at topic creation** (recommended for fault tolerance)
- Each broker acts as **leader for some partitions, follower for others**
- **Followers passively replicate leader**
- If leader fails, **follower automatically becomes new leader**

### Message Replication Process
1. Producer configured with list of brokers
2. Producer asks first available broker for leader of desired partition
3. Producer sends message to leader
4. **Leader writes to local log**
5. **Each follower writes to own log**
6. After acknowledgments from followers, **message is committed**

## Kafka Command Line Tools

### Creating Topics
```bash
kafka-topics --create \
--zookeeper zkhost1:2181,zkhost2:2181,zkhost3:2181 \
--replication-factor 3 \
--partitions 5 \
--topic device_status
```

### Listing Topics
```bash
kafka-topics --list \
--zookeeper zkhost1:2181,zkhost2:2181,zkhost3:2181
```

### Running Producer
```bash
kafka-console-producer \
--broker-list brokerhost1:9092,brokerhost2:9092 \
--topic device_status
```

### Reading File to Topic
```bash
cat alerts.txt | kafka-console-producer \
--broker-list brokerhost1:9092,brokerhost2:9092 \
--topic device_status
```

### Running Consumer
```bash
kafka-console-consumer \
--zookeeper zkhost1:2181,zkhost2:2181,zkhost3:2181 \
--topic device_status \
--from-beginning
```

## Apache Flume

### What is Apache Flume?
- **High-performance system for data collection**
- Originally for near-real time log data ingestion
- Now used for **any streaming event data collection**
- Supports **aggregating data from many sources into HDFS**

### Benefits
- **Horizontally-scalable**
- **Extensible**
- **Reliable**

### Flume Events
- **Fundamental unit of data** in Flume
- Consists of:
  - **Body (payload)**
  - **Collection of headers (metadata)**
- Headers are **name-value pairs** mainly used for directing output

### Core Architecture Components

#### Source
- **Receives events** from external actor that generates them

#### Sink
- **Sends event to destination**

#### Channel
- **Buffers events** from source until drained by sink

#### Agent
- **Configures and hosts** source, channel, and sink
- **Java process running in JVM**

### Flume Data Flow Example (Syslog to HDFS)
1. Server running syslog daemon logs message
2. Flume agent with syslog source retrieves event
3. Source pushes event to channel (buffered in memory)
4. Sink pulls data from channel and writes to HDFS

## Built-in Flume Components

### Sources
- **Syslog**: Captures messages from UNIX syslog daemon over network
- **Netcat**: Captures data written to socket on arbitrary TCP port
- **Exec**: Executes UNIX program, reads events from standard output
- **Spooldir**: Extracts events from files in specified local directory
- **HTTP Source**: Retrieves events from HTTP requests
- **Kafka**: Retrieves events by consuming messages from Kafka topic

### Sinks
- **Null**: Discards all events (equivalent of /dev/null)
- **Logger**: Logs event to INFO level using SLF4J
- **IRC**: Sends event to specified Internet Relay Chat channel
- **HDFS**: Writes event to file in specified HDFS directory
- **Kafka**: Sends event as message to Kafka topic
- **HBaseSink**: Stores event in HBase

### Channels
- **Memory**: Stores events in RAM (extremely fast but not reliable - volatile)
- **File**: Stores events on local disk (slower than RAM but more reliable)
- **Kafka**: Uses Kafka as scalable, reliable, highly available channel

## Flume Configuration

### Configuration File Structure
- Uses **Java properties file**
- **Hierarchical references** with user-defined IDs
- Can configure **multiple agents in single file**

### Example Configuration
```properties
# Define components for agent named 'agent1'
agent1.sources = mysource
agent1.sinks = mysink
agent1.channels = mychannel

# Source properties
agent1.sources.mysource.prop1 = pp1

# Sink properties  
agent1.sinks.mysink.prop2 = pp2
```

### Complete Example (Spool Directory to HDFS)
```properties
agent1.sources = src1
agent1.sinks = sink1
agent1.channels = ch1

agent1.channels.ch1.type = memory

agent1.sources.src1.type = spooldir
agent1.sources.src1.spoolDir = /var/flume/incoming
agent1.sources.src1.channels = ch1

agent1.sinks.sink1.type = hdfs
agent1.sinks.sink1.hdfs.path = /loudacre/logdata
agent1.sinks.sink1.channel = ch1
```

### Starting Flume Agent
```bash
flume-ng agent \
--conf /etc/flume-ng/conf \
--conf-file /path/to/flume.conf \
--name agent1 \
-Dflume.root.logger=INFO,console
```

## Key Integration Points
- **Kafka and Flume integration**: Flume can use Kafka as both source and sink
- **Large ecosystem**: Both tools integrate with Apache Spark, AWS, and other big data tools
- **Client libraries**: Available in multiple languages (Python, PHP, C/C++, Go, .NET, Ruby)

## Exam Key Points to Remember
- **Kafka**: Distributed commit log, topic partitioning for scalability, consumer groups, ZooKeeper dependency
- **Message ordering**: Preserved per partition, NOT across entire topic
- **Replication**: Leader-follower model, automatic failover
- **Flume**: Agent-based architecture, source-channel-sink pattern, event-driven processing
- **Channel types**: Memory (fast, volatile) vs File (slower, reliable) vs Kafka (scalable, reliable)
- **Command line tools**: Different connection requirements (brokers vs ZooKeeper)