## Big Data Infrastructure
- **Clusters**: Groups of computers (servers) connected by networks
- **Network Architecture**: Typical two-level network architecture for clusters
- **Distributed Processing**: Key layers include Storage, Resource Management, and Distributed Processing

## Hadoop Core Architecture

### Three Major Layers
1. **Storage** (HDFS)
2. **Resource Management** (YARN)
3. **Distributed Processing** (MapReduce, Spark, etc.)

### Cluster Terminology
- **Cluster**: Group of computers working together for data storage, processing, and resource management
- **Node**: Individual computer in cluster
  - **Master nodes**: Manage distribution of work and data to worker nodes
  - **Worker nodes**: Execute tasks assigned by master nodes
- **Daemon**: Program running on a node, each performing different cluster functions
## Common Hadoop Use Cases
- Extract, Transform, and Load (ETL)
- Data analysis and text mining
- Index building and graph creation/analysis
- Pattern recognition and collaborative filtering
- Prediction models and sentiment analysis
- Risk assessment and data storage

## Hadoop Distributions
- **Cloudera Distribution**: Popular enterprise Hadoop distribution
- **Hue (Hadoop User Experience)**: Web front-end for Hadoop
  - Upload/browse HDFS data
  - Query tables in Impala and Hive
  - Run Spark jobs, Pig jobs, Oozie workflows
  - Build Cloudera Search dashboards
  - 100% open source, Apache licensed

## HDFS (Hadoop Distributed File System)

### Basic Concepts
- **Java-based** file system based on <mark style="background: #FF5582A6;">Google File System</mark>
- Sits on top of native file systems (ext3, ext4, xfs)
- Provides **redundant storage** for massive data amounts
- Optimized for **large, streaming reads** rather than random reads
- **"Write once, read-many"** - no random writes allowed
- Best with **modest number of large files** (millions, not billions)
- Each file typically **100MB or more**

**Master: NameNode**
- Manages file system namespace and metadata
- Stores **FsImage** and **EditLog**
- Regulates client access to files
- **Must run at all times** - cluster becomes inaccessible if stops

**Slave: DataNode**
- Many per cluster
- Manages storage attached to nodes
- Periodically reports status to NameNode

### HDFS Blocks
- Data files split into **blocks (default 128MB)**
- Distributed at load time
- Each block **replicated on multiple DataNodes (default 3x)**
- NameNode stores metadata
- Example: 210MB file = 2 blocks (128MB + 82MB)
- **Block replication factor** configurable per file

### <mark style="background: #ABF7F7A6;">High Availability Options</mark>
<mark style="background: #BBFABBA6;">**High Availability Setup**</mark>:
- Two NameNodes: Active and Standby
- Standby automatically takes over if Active fails

<mark style="background: #FF5582A6;">**Classic Mode** (small clusters):</mark>
- One NameNode
- One Secondary NameNode (bookkeeping, NOT backup)

### Accessing HDFS
1. **Command Line**: `hdfs dfs`
2. **Spark**: By URI (hdfs://nnhost:port/file...)
3. **Other Programs**: Java API, RESTful interface

### Key HDFS Commands
```bash
# Copy file to HDFS
hdfs dfs -put foo.txt

# List directories
hdfs dfs -ls
hdfs dfs -ls /

# Display file contents
hdfs dfs -cat /user/fred/bar.txt

# Copy from HDFS to local
hdfs dfs -get bar.txt baz.txt

# Create directory
hdfs dfs -mkdir input

# Delete operations
hdfs dfs -rm filename
hdfs dfs -rm input_old/*
hdfs dfs -rm -r input_old
```

### HDFS Best Practices
**Directory Structure**:
- `/user/...` - individual user data/config
- `/etl` - Extract/Transform/Load work in progress
- `/tmp` - temporary shared data
- `/data` - processed datasets for organization analysis
- `/app` - non-data files (config, JAR, SQL files)

## Data Storage Formats

### Text Files
- Human-readable
- Inefficient for large datasets

### SequenceFiles
- Store key-value pairs in binary format
- Less verbose, more efficient than text
- Can store binary data (images)
- **Java-specific**, tightly coupled to Hadoop
- **Verdict**: Better performance, poor interoperability

### Apache Avro
- **Efficient binary encoding**
- Widely supported in Hadoop ecosystem
- **Schema embedded in file** (JSON format)
- **Schema evolution** accommodates changes
- Multi-language support
- **Verdict**: Excellent interoperability and performance
- **Best choice for general-purpose storage**

**Avro Tools**:
```bash
# Read data
avro-tools tojson mydatafile.avro

# Read schema
avro-tools getschema mydatafile.avro
```

### Apache Parquet
- **Columnar format** (organizes by column, not row)
- Developed by Cloudera and Twitter
- Supported in Spark, MapReduce, Hive, Pig, Impala
- **Schema metadata embedded**
- Uses Google Dremel paper optimizations
- **Most efficient for column-based access patterns**
- **Verdict**: <mark style="background: #FF5582A6;">Excellent for analytical queries selecting subset of columns</mark>

**Parquet Tools**:
```bash
# Read data
parquet-tools head mydatafile.parquet

# Read schema
parquet-tools schema mydatafile.parquet
```

### Compression
- Reduces disk space requirements
- **Codec**: Implementation of compression algorithm
- Various codecs available for different file formats
## MapReduce
### Programming Model
**Map Step**:
- Input split into pieces
- Worker nodes process pieces in parallel
- Process key-value pairs
- Results stored in local file system

**Reduce Step**:

- Data aggregated from map steps
- Multiple reduce tasks can parallelize aggregation
- Resource Manager controls the process
### Map Tasks and Data Locality
- **Data-local**: Task runs on node with data
- **Rack-local**: Task runs on same rack as data
- **Off-rack**: Task runs on different rack

### MapReduce Phases

**Map Phase**:
- Small programs distributed across cluster
- Each mapper gets portion of input (split)
- Parses, filters, or transforms input
- Produces grouped `<key,value>` pairs

**Shuffle Phase**:
- Output grouped by key locally
- One node chosen per unique key
- Data movement orchestrated by MapReduce

**Reduce Phase**:
- Aggregates all values for assigned keys
- Each reducer writes to own output file

### Optional Combiner
- Local aggregation in Map task
- Reduces network data transfer
- Reduces merge effort
- Runs after Map task, before Shuffle

### MapReduce Configuration
- **Local Execution**: Hadoop attempts local split execution
- **Number of Map Tasks**: Configurable, depends on file splittability
- **Number of Reduce Tasks**: Usually fewer than Map tasks
- **Redundant Execution**: Multiple Map tasks per split (first to finish wins)

### Required Hadoop Packages
- hadoop-hdfs.jar
- hadoop-common.jar
- hadoop-mapreduce-client-common.jar
- hadoop-mapreduce-client-core.jar

## YARN (Yet Another Resource Negotiator)

### What is YARN?
- Hadoop processing layer
- Contains **resource manager** and **job scheduler**
- Separates resource management from job scheduling

### YARN Daemons

**ResourceManager (RM)**:
- Runs on master node
- Global resource scheduler
- Arbitrates system resources between competing applications

**NodeManager (NM)**:
- Runs on worker nodes
- Communicates with ResourceManager
- Manages node resources
- Launches containers

### Key YARN Components

**Containers**:
- Allocate specific resources (memory, CPU cores) on worker nodes
- Applications run in one or more containers
- Clients request containers from ResourceManager

**ApplicationMaster (AM)**:
- One per application
- Framework/application specific
- Runs in container
- Requests additional containers for application tasks

### YARN Application Execution Flow
1. Client submits application to ResourceManager
2. ResourceManager allocates container for ApplicationMaster
3. ApplicationMaster registers with ResourceManager
4. ApplicationMaster requests containers for tasks
5. ResourceManager allocates containers on NodeManagers
6. ApplicationMaster launches tasks in containers
7. Tasks report progress to ApplicationMaster
8. ApplicationMaster reports to ResourceManager

### YARN Management Tools

**Hue Job Browser**:
- Monitor job status
- View logs
- Kill running jobs

**YARN Web UI**:
- ResourceManager UI (port 8088)
- URL: http://hostname:8088/cluster
- More detailed view than Hue
- No control/configuration capabilities

**YARN Command Line**:
```bash
# List running applications
yarn application -list

# Kill application
yarn application -kill <application_id>

# View application logs
yarn logs -applicationId <application_id>

# View command options
yarn --help
```

## ZooKeeper

### Overview
- **Distributed, open-source coordination service**
- Provides primitives for distributed applications
- Enables synchronization, configuration maintenance, groups, and naming
- Simple set of operations on small data nodes

### ZooKeeper Use Cases
Used by:
- Apache HBase
- Apache Kafka
- Apache Solr
- Yahoo! Fetching Service
- Facebook Messages

### Master-Worker Architecture Problems
ZooKeeper solves three key problems:
1. **Master crashes**
2. **Worker crashes**
3. **Communication failures**

### ZooKeeper Data Model

**Znodes**:
- Small data nodes organized in tree structure
- Root node can contain multiple child nodes
- Leaf nodes contain actual data

### ZooKeeper Operations
```bash
# Create znode
create /path data

# Delete znode
delete /path

# Check existence
exists /path

# Set data
setData /path data

# Get data
getData /path

# Get children
getChildren /path
```

### Znode Types

**Persistent vs Ephemeral**:
- **Persistent**: Only deleted through explicit delete call
- **Ephemeral**: Deleted when client crashes or closes connection

**Sequential Znodes**:
- Assigned unique, monotonically increasing integer
- Sequence number appended to path
- Example: `/tasks/task-1`, `/tasks/task-2`

### Watches and Notifications
- Replaces client polling with notification mechanism
- Clients register to receive change notifications
- API calls can pass Watcher object or use default watcher

### ZooKeeper Architecture

**Deployment Modes**:
- **Standalone**: Single server, no state replication
- **Quorum**: Group of servers (ensemble) replicates state

**Sessions**:
- Client establishes session with service
- **FIFO order guarantees** (First In, First Out)
- Sessions can migrate between servers
- Various session states and transitions

### ZooKeeper Quorums
- Multiple servers ensure high availability
- Client can reconnect if one server fails
- Maintains consistency across ensemble

## Key Exam Points to Remember

1. **HDFS is write-once, read-many** with 128MB default block size
2. **NameNode failure** makes entire cluster inaccessible
3. **Default replication factor is 3**
4. **Avro and Parquet** are best choices for Hadoop storage
5. **YARN separates** resource management from job scheduling
6. **MapReduce has 3 phases**: Map, Shuffle, Reduce
7. **ZooKeeper provides coordination** for distributed systems
8. **Ephemeral znodes disappear** when client disconnects
9. **Columnar formats** (Parquet) efficient for analytical queries
10. **Hue provides web interface** for Hadoop ecosystem